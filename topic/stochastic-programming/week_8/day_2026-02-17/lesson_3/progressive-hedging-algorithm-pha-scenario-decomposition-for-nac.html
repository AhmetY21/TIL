<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Progressive Hedging Algorithm (PHA): scenario decomposition for NAC</title>
  <script>
    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
  </script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }

    .page-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 24px;
      padding-bottom: 16px;
      border-bottom: 1px solid #e5e7eb;
    }
    .back-link {
      font-weight: 600;
      text-decoration: none;
    }
    .back-link:hover {
      text-decoration: underline;
    }

    .theme-toggle {
      background: none;
      border: none;
      cursor: pointer;
      font-size: 1.5rem;
      padding: 8px;
      border-radius: 50%;
      transition: background 0.2s;
    }
    .theme-toggle:hover {
      background: rgba(0,0,0,0.05);
    }
    .dark .theme-toggle:hover {
      background: rgba(255,255,255,0.1);
    }

    .dark body {
      background: #0f172a;
      color: #e2e8f0;
    }
    .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
    .dark a { color: #60a5fa; }
    .dark .page-header { border-bottom-color: #334155; }
    .dark code { background-color: #1e293b; color: #e2e8f0; }
    .dark pre {
      border: 1px solid #334155;
    }
    .dark blockquote {
      border-left-color: #334155;
      color: #94a3b8;
    }
    .dark th, .dark td { border-color: #334155; }
    .dark th { background: #1e293b; }
    .dark hr { border-top-color: #334155; }

    /* Copy Button */
    pre { position: relative; }
    .copy-button {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      font-size: 0.8rem;
      color: #94a3b8;
      background: rgba(255, 255, 255, 0.1);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 4px;
      cursor: pointer;
      opacity: 0;
      transition: opacity 0.2s, background 0.2s;
    }
    pre:hover .copy-button, .copy-button:focus {
      opacity: 1;
    }
    .copy-button:hover {
      background: rgba(255, 255, 255, 0.2);
      color: #e2e8f0;
    }

    /* Skip Link */
    .skip-link {
      position: absolute;
      top: -40px;
      left: 0;
      background: #0f172a;
      color: white;
      padding: 8px 16px;
      z-index: 100;
      transition: top 0.2s;
      font-weight: 600;
      border-bottom-right-radius: 6px;
    }
    .skip-link:focus {
      top: 0;
    }

  </style>
</head>
<body>
  <a href="#main-content" class="skip-link">Skip to content</a>

  <div class="page-header">
    <a href="../../../../../hubs/stochastic-programming-index.html" class="back-link">‚Üê Back to Stochastic Programming</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle Dark Mode">üåô</button>
  </div>
  <main id="main-content">

<h1 id="topic-progressive-hedging-algorithm-pha-scenario-decomposition-for-nac">Topic: Progressive Hedging Algorithm (PHA): scenario decomposition for NAC</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>The Progressive Hedging Algorithm (PHA) is a scenario decomposition method used to solve stochastic programming problems, particularly those with Non-Anticipativity Constraints (NAC). Stochastic programming involves optimization problems where some parameters are uncertain and described by probability distributions. These distributions are often represented by a set of discrete scenarios. The NAC is a crucial component; it mandates that decisions made <em>before</em> the realization of uncertainty (first-stage decisions) must be the same across all scenarios.  In essence, it means we can't base our upfront decisions on information we don't yet have.</p>
<p><strong>What is it?</strong></p>
<p>PHA breaks down a large, complex stochastic program into a set of smaller, easier-to-solve subproblems, one for each scenario. It then iteratively coordinates the solutions of these subproblems to enforce the non-anticipativity constraint. This is done by penalizing deviations of the first-stage decisions across scenarios.  The algorithm aims to find a common first-stage decision that performs well across all scenarios by iteratively adjusting the solutions of individual scenario problems.</p>
<p><strong>How can we use it?</strong></p>
<p>Here's a breakdown of the iterative process:</p>
<ol>
<li><strong>Initialization:</strong></li>
<li>Generate a set of scenarios, each with an associated probability.</li>
<li>Initialize the first-stage decision variables (denoted as <code>x</code>) for each scenario.  A common starting point is often used to ease the initial stages, but this is not required.</li>
<li>
<p>Set penalty parameters (also known as augmented Lagrangian multipliers, denoted as <code>œÅ</code>) for the non-anticipativity constraints to initial values (often small positive values).</p>
</li>
<li>
<p><strong>Iteration (repeat until convergence):</strong></p>
</li>
<li><strong>Scenario Decomposition:</strong> Solve each scenario problem independently, given the current first-stage decision variables (<code>x</code>) and penalty parameters (<code>œÅ</code>). Each scenario problem typically includes the scenario-specific objective function, second-stage decisions, and constraints, <em>plus</em> a penalty term related to the difference between the scenario's first-stage decision and the average first-stage decision across all scenarios.</li>
<li><strong>Averaging:</strong> Compute the average of the first-stage decision variables across all scenarios. This provides an estimate of the "consensus" decision.  Let <code>x_s</code> denote the first-stage decision for scenario <code>s</code>, then the average is <code>x_bar = sum(p_s * x_s)</code> where <code>p_s</code> is the probability of scenario <code>s</code>.</li>
<li><strong>Updating First-Stage Decisions:</strong> For each scenario, update the first-stage decision variables based on the scenario solution and the average decision. This update typically involves shifting the first-stage decision towards the average, weighted by the penalty parameter <code>œÅ</code>.</li>
<li>
<p><strong>Updating Penalty Parameters (Augmented Lagrangian multipliers):</strong> Update the penalty parameters <code>œÅ</code>.  This usually involves increasing <code>œÅ</code> when the non-anticipativity constraints are violated (i.e., the difference between the scenario's first-stage decision and the average decision is above a certain tolerance).</p>
</li>
<li>
<p><strong>Convergence:</strong> Check for convergence. Convergence is usually determined by checking if the difference between the first-stage decisions across scenarios is small enough (i.e., the non-anticipativity constraints are approximately satisfied) and/or if the objective function value is not changing significantly between iterations.</p>
</li>
</ol>
<p>PHA is particularly useful for problems with a large number of scenarios where solving the entire stochastic program at once is computationally intractable. The decomposition allows for parallelization, making it suitable for high-performance computing environments.</p>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Consider a power generation company that needs to decide how much to invest in different types of energy sources (e.g., solar, wind, natural gas) to meet future electricity demand. The future electricity demand and fuel prices are uncertain and represented by a set of scenarios.</p>
<ul>
<li><strong>First-Stage Decision (x):</strong> Investment decisions (e.g., capacity of each energy source to build). These decisions must be made <em>now</em>, before we know the actual future demand and prices.</li>
<li><strong>Second-Stage Decisions (y):</strong> Operational decisions (e.g., how much electricity to generate from each source in each period). These decisions are made <em>after</em> the scenario is revealed and can be scenario-dependent.</li>
<li><strong>Uncertainty:</strong> Future electricity demand, fuel prices, and renewable energy availability.</li>
<li><strong>Objective:</strong> Minimize the total cost of investment and operation over a planning horizon, subject to meeting demand in each scenario.</li>
<li><strong>Non-Anticipativity Constraint:</strong> The investment decisions (first-stage variables) must be the same across all scenarios.  We can't build a different power plant portfolio based on what we <em>think</em> might happen in each scenario.</li>
</ul>
<p>Applying PHA to this problem involves:</p>
<ol>
<li>Creating a set of scenarios representing different possible realizations of future demand and fuel prices.</li>
<li>Solving a separate optimization problem for each scenario, where the objective is to minimize the cost of meeting demand in that scenario, subject to the investment decisions (first-stage variables) being equal to some initial guess.</li>
<li>Averaging the investment decisions across all scenarios.</li>
<li>Updating the investment decisions for each scenario based on the average decision and the penalty parameters.</li>
<li>Updating the penalty parameters.</li>
<li>Repeating steps 2-5 until the investment decisions converge across scenarios.</li>
</ol>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<p>Here's a simplified Python example using the <code>Pyomo</code> library for optimization. Note that this is a high-level illustration and would require more detailed modeling for a real-world application.  This example assumes that the scenario generation has already occurred and that we are starting from a set of scenarios ready for PHA.  Key aspects like solver choice, parameter tuning, and more complex convergence criteria are omitted for clarity.</p>
<pre class="codehilite"><code class="language-python">import pyomo.environ as pyo
import numpy as np

def progressive_hedging(scenarios, rho_initial=1.0, rho_update_factor=1.1, tolerance=1e-6, max_iterations=100):
    &quot;&quot;&quot;
    Implements the Progressive Hedging Algorithm.

    Args:
        scenarios (list): A list of Pyomo model instances, one for each scenario.
                         Each model should have first-stage variables named 'x' and
                         an objective function.
        rho_initial (float): Initial value for the penalty parameter (rho).
        rho_update_factor (float): Factor by which to increase rho if NAC is violated.
        tolerance (float): Convergence tolerance.
        max_iterations (int): Maximum number of iterations.

    Returns:
        list: List of updated Pyomo model instances, one for each scenario, after PHA.
    &quot;&quot;&quot;

    num_scenarios = len(scenarios)
    probabilities = [1/num_scenarios] * num_scenarios  # Assuming equal probabilities
    rho = rho_initial
    x_avg = None #initialize the average, it will be calculated at the first iteration

    for iteration in range(max_iterations):
        print(f&quot;Iteration {iteration+1}/{max_iterations}&quot;)

        # 1. Solve scenario problems
        x_solutions = []
        for s in range(num_scenarios):
            # Add augmented Lagrangian term to the objective function:
            # Minimize OriginalObjective + rho/2 * sum((x[i] - x_avg[i])**2)
            # + sum(dual_vars[i] * (x[i] - x_avg[i])) #Dual Variable
            model = scenarios[s]
            if iteration &gt; 0:
                penalty_term = 0
                for var in model.x:  # Assuming 'x' is a Set for the first stage vars
                    penalty_term += rho/2 * (model.x[var] - x_avg[var])**2

                model.objective.expr = model.objective.expr + penalty_term

            # Solve the problem
            solver = pyo.SolverFactory('glpk') #or any other solver
            solver.solve(model)
            x_solutions.append({v: pyo.value(model.x[v]) for v in model.x})

        # 2. Calculate average first-stage decision
        x_avg = {}
        for var in scenarios[0].x: #assumes all models share the same first-stage variables
            x_avg[var] = sum(probabilities[s] * x_solutions[s][var] for s in range(num_scenarios))


        # 3. Check for convergence
        max_deviation = 0
        for s in range(num_scenarios):
            for var in scenarios[0].x:
                deviation = abs(x_solutions[s][var] - x_avg[var])
                max_deviation = max(max_deviation, deviation)


        print(f&quot;Max deviation: {max_deviation}&quot;)
        if max_deviation &lt; tolerance:
            print(&quot;Convergence achieved!&quot;)
            break

        # 4. Update rho (if needed - omitted here for simplicity, see follow-up)
        if max_deviation &gt; tolerance:
            rho *= rho_update_factor #Increase rho
            print(f&quot;Updating rho to {rho}&quot;)
        else:
            print(&quot;no penalty update, small max deviation&quot;)


    return scenarios

# Example Usage (replace with your actual models):
# First, define a sample Stochastic Problem with two stages
def create_scenario_model(scenario_data): # a scenario could be a single dictionary, as you define it
  model = pyo.ConcreteModel()

  # Index set for first-stage variables (example: product types)
  model.PRODUCTS = pyo.Set(initialize=['A', 'B'])

  # First-stage variables (e.g., production quantities)
  model.x = pyo.Var(model.PRODUCTS, within=pyo.NonNegativeReals, name='x')

  # Second-stage variables (e.g., sales quantities)
  model.y = pyo.Var(model.PRODUCTS, within=pyo.NonNegativeReals)

  # Parameters (scenario-dependent)
  model.demand = pyo.Param(model.PRODUCTS, initialize=scenario_data['demand'])
  model.price = pyo.Param(model.PRODUCTS, initialize=scenario_data['price'])
  model.production_cost = pyo.Param(model.PRODUCTS, initialize=scenario_data['production_cost'])

  # Objective function
  def objective_rule(model):
      production_costs = sum(model.production_cost[p] * model.x[p] for p in model.PRODUCTS)
      revenues = sum(model.price[p] * model.y[p] for p in model.PRODUCTS)
      return production_costs - revenues

  model.objective = pyo.Objective(rule=objective_rule, sense=pyo.minimize)

  # Constraints
  def sales_limit_rule(model, p):
      return model.y[p] &lt;= model.demand[p]
  model.sales_limit = pyo.Constraint(model.PRODUCTS, rule=sales_limit_rule)

  def production_limit_rule(model, p):
    return model.y[p] &lt;= model.x[p]
  model.production_limit = pyo.Constraint(model.PRODUCTS, rule = production_limit_rule)

  return model

# Create a dummy set of scenarios
scenario_data1 = {'demand': {'A': 10, 'B': 15}, 'price': {'A': 5, 'B': 7}, 'production_cost':{'A':1, 'B':2}}
scenario_data2 = {'demand': {'A': 12, 'B': 13}, 'price': {'A': 6, 'B': 6}, 'production_cost':{'A':1.5, 'B':2.5}}
scenarios = [create_scenario_model(scenario_data1), create_scenario_model(scenario_data2)]

# Run PHA
updated_scenarios = progressive_hedging(scenarios)

# Print results for the first scenario (for demonstration)
print(&quot;\nResults for Scenario 1:&quot;)
for var in updated_scenarios[0].x:
    print(f&quot;x[{var}]: {pyo.value(updated_scenarios[0].x[var])}&quot;)
</code></pre>

<p><strong>Important notes:</strong></p>
<ul>
<li>This code provides a conceptual illustration. A robust implementation requires careful tuning of parameters like <code>rho_initial</code>, <code>rho_update_factor</code>, and <code>tolerance</code>.</li>
<li>The specific structure of your <code>scenarios</code> list will depend on how your stochastic program is modeled using Pyomo.  The code assumes the first-stage variables are accessed via <code>model.x</code>.</li>
<li>This example uses <code>glpk</code> solver which is not the most efficient, consider other solvers.</li>
<li>Consider a more sophisticated method for updating <code>rho</code> like using a dual variable.</li>
<li>Consider implementing a non-increasing penalty for augmented lagrangian (e.g., see https://link.springer.com/article/10.1007/s10589-022-00404-x)</li>
</ul>
<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>How can I implement a more sophisticated rho update scheme that utilizes the dual variables from solving each scenario's subproblem? This would allow for a more dynamic adjustment of the penalty parameter based on the actual violation of the non-anticipativity constraints, potentially leading to faster convergence. Show how the Augmented Lagrangian Dual variable would be defined in <code>pyomo</code> within the above <code>create_scenario_model</code> definition. Then, show how to update rho based on a dual variable within the <code>progressive_hedging</code> function.</p>
<p>To answer the question, the augmented lagrangian would be set up using the first stage variables. In this example, they are indexed by <code>model.PRODUCTS</code> as shown above. Add the following to the function:
1) Augmented Lagrangian variable in the <code>create_scenario_model</code>
2) Include the <code>dual_vars</code> as part of solving the objective
3) Update the rho parameter in the main loop using the dual variables.</p>
<pre class="codehilite"><code class="language-python">import pyomo.environ as pyo
import numpy as np

def progressive_hedging(scenarios, rho_initial=1.0, rho_update_factor=1.1, tolerance=1e-6, max_iterations=100, dual_update_step = 0.1):
    &quot;&quot;&quot;
    Implements the Progressive Hedging Algorithm with dual variable updates.

    Args:
        scenarios (list): A list of Pyomo model instances, one for each scenario.
                         Each model should have first-stage variables named 'x' and
                         an objective function, and dual variables 'dual'.
        rho_initial (float): Initial value for the penalty parameter (rho).
        rho_update_factor (float): Factor by which to increase rho if NAC is violated.
        tolerance (float): Convergence tolerance.
        max_iterations (int): Maximum number of iterations.
        dual_update_step (float): Step size for updating dual variables.

    Returns:
        list: List of updated Pyomo model instances, one for each scenario, after PHA.
    &quot;&quot;&quot;

    num_scenarios = len(scenarios)
    probabilities = [1/num_scenarios] * num_scenarios  # Assuming equal probabilities
    rho = rho_initial
    x_avg = None #initialize the average, it will be calculated at the first iteration

    for iteration in range(max_iterations):
        print(f&quot;Iteration {iteration+1}/{max_iterations}&quot;)

        # 1. Solve scenario problems
        x_solutions = []
        for s in range(num_scenarios):
            # Add augmented Lagrangian term to the objective function:
            # Minimize OriginalObjective + rho/2 * sum((x[i] - x_avg[i])**2)
            # + sum(dual_vars[i] * (x[i] - x_avg[i])) #Dual Variable
            model = scenarios[s]
            if iteration &gt; 0:
                penalty_term = 0
                dual_term = 0
                for var in model.x:  # Assuming 'x' is a Set for the first stage vars
                    penalty_term += rho/2 * (model.x[var] - x_avg[var])**2
                    dual_term += model.dual[var] * (model.x[var] - x_avg[var])

                model.objective.expr = model.objective.expr + penalty_term + dual_term

            # Solve the problem
            solver = pyo.SolverFactory('glpk') #or any other solver
            solver.solve(model)
            x_solutions.append({v: pyo.value(model.x[v]) for v in model.x})

        # 2. Calculate average first-stage decision
        x_avg = {}
        for var in scenarios[0].x: #assumes all models share the same first-stage variables
            x_avg[var] = sum(probabilities[s] * x_solutions[s][var] for s in range(num_scenarios))


        # 3. Check for convergence
        max_deviation = 0
        for s in range(num_scenarios):
            for var in scenarios[0].x:
                deviation = abs(x_solutions[s][var] - x_avg[var])
                max_deviation = max(max_deviation, deviation)


        print(f&quot;Max deviation: {max_deviation}&quot;)
        if max_deviation &lt; tolerance:
            print(&quot;Convergence achieved!&quot;)
            break

        # 4. Update dual variables
        for s in range(num_scenarios):
          model = scenarios[s]
          for var in model.x:
              model.dual[var] = model.dual[var] + rho * (pyo.value(model.x[var]) - x_avg[var])


        # 5. Update rho (if needed - omitted here for simplicity, see follow-up)
        if max_deviation &gt; tolerance:
            rho *= rho_update_factor #Increase rho
            print(f&quot;Updating rho to {rho}&quot;)
        else:
            print(&quot;no penalty update, small max deviation&quot;)


    return scenarios

# Example Usage (replace with your actual models):
# First, define a sample Stochastic Problem with two stages
def create_scenario_model(scenario_data): # a scenario could be a single dictionary, as you define it
  model = pyo.ConcreteModel()

  # Index set for first-stage variables (example: product types)
  model.PRODUCTS = pyo.Set(initialize=['A', 'B'])

  # First-stage variables (e.g., production quantities)
  model.x = pyo.Var(model.PRODUCTS, within=pyo.NonNegativeReals, name='x')

  # Second-stage variables (e.g., sales quantities)
  model.y = pyo.Var(model.PRODUCTS, within=pyo.NonNegativeReals)

  # Augmented Lagrangian dual variables
  model.dual = pyo.Var(model.PRODUCTS, initialize=0, name='dual') #initialize dual to 0

  # Parameters (scenario-dependent)
  model.demand = pyo.Param(model.PRODUCTS, initialize=scenario_data['demand'])
  model.price = pyo.Param(model.PRODUCTS, initialize=scenario_data['price'])
  model.production_cost = pyo.Param(model.PRODUCTS, initialize=scenario_data['production_cost'])

  # Objective function
  def objective_rule(model):
      production_costs = sum(model.production_cost[p] * model.x[p] for p in model.PRODUCTS)
      revenues = sum(model.price[p] * model.y[p] for p in model.PRODUCTS)
      return production_costs - revenues

  model.objective = pyo.Objective(rule=objective_rule, sense=pyo.minimize)

  # Constraints
  def sales_limit_rule(model, p):
      return model.y[p] &lt;= model.demand[p]
  model.sales_limit = pyo.Constraint(model.PRODUCTS, rule=sales_limit_rule)

  def production_limit_rule(model, p):
    return model.y[p] &lt;= model.x[p]
  model.production_limit = pyo.Constraint(model.PRODUCTS, rule = production_limit_rule)

  return model

# Create a dummy set of scenarios
scenario_data1 = {'demand': {'A': 10, 'B': 15}, 'price': {'A': 5, 'B': 7}, 'production_cost':{'A':1, 'B':2}}
scenario_data2 = {'demand': {'A': 12, 'B': 13}, 'price': {'A': 6, 'B': 6}, 'production_cost':{'A':1.5, 'B':2.5}}
scenarios = [create_scenario_model(scenario_data1), create_scenario_model(scenario_data2)]

# Run PHA
updated_scenarios = progressive_hedging(scenarios)

# Print results for the first scenario (for demonstration)
print(&quot;\nResults for Scenario 1:&quot;)
for var in updated_scenarios[0].x:
    print(f&quot;x[{var}]: {pyo.value(updated_scenarios[0].x[var])}&quot;)
</code></pre>

<p><strong>Explanation of the changes:</strong></p>
<ol>
<li>
<p><strong>Augmented Lagrangian Dual Variables:</strong></p>
<ul>
<li>In <code>create_scenario_model</code>, a new <code>pyo.Var</code> called <code>dual</code> is added, indexed by <code>model.PRODUCTS</code>. This represents the dual variables associated with the non-anticipativity constraints for each first-stage variable. The initial value is set to 0.</li>
</ul>
</li>
<li>
<p><strong>Including the Dual Term in the Objective:</strong></p>
<ul>
<li>In <code>progressive_hedging</code>, within the main loop, the <code>dual_term</code> is added to the objective function of each scenario. The <code>dual_term</code> is computed as the sum of <code>model.dual[var] * (model.x[var] - x_avg[var])</code> for each first-stage variable <code>var</code>.</li>
<li>The complete objective function now includes the original objective, the quadratic penalty term (augmented Lagrangian penalty), and the dual term.</li>
</ul>
</li>
<li>
<p><strong>Updating Dual Variables:</strong></p>
<ul>
<li>After solving each scenario and checking for convergence, the dual variables are updated using the following formula: <code>model.dual[var] = model.dual[var] + rho * (pyo.value(model.x[var]) - x_avg[var])</code>. This is a standard dual update rule, where the dual variable is adjusted based on the violation of the non-anticipativity constraint (the difference between the scenario's first-stage variable and the average).</li>
</ul>
</li>
</ol>
<p>This implementation should enable a more adaptive penalty adjustment, potentially leading to faster convergence compared to a fixed or simply increasing <code>rho</code>. However, parameter tuning remains crucial for optimal performance. Specifically, the <code>dual_update_step</code> should be tested. Also, since this requires computing the dual variable, the appropriate pyomo solver that computes it must be selected.</p>
    </main>
<script>
    const btn = document.getElementById('theme-toggle');
    const html = document.documentElement;
    
    function updateIcon() {
      btn.textContent = html.classList.contains('dark') ? '‚òÄÔ∏è' : 'üåô';
    }
    
    // Set initial icon
    updateIcon();

    btn.addEventListener('click', () => {
      if (html.classList.contains('dark')) {
        html.classList.remove('dark');
        localStorage.theme = 'light';
      } else {
        html.classList.add('dark');
        localStorage.theme = 'dark';
      }
      updateIcon();
    });

    // Copy Code Functionality
    document.querySelectorAll('pre').forEach(pre => {
      const button = document.createElement('button');
      button.className = 'copy-button';
      button.textContent = 'Copy';
      button.setAttribute('aria-label', 'Copy code to clipboard');

      pre.appendChild(button);

      button.addEventListener('click', async () => {
        const code = pre.querySelector('code');
        if (!code) return;

        try {
          await navigator.clipboard.writeText(code.innerText);
          button.textContent = 'Copied!';
          setTimeout(() => {
            button.textContent = 'Copy';
          }, 2000);
        } catch (err) {
          console.error('Failed to copy:', err);
          button.textContent = 'Error';
        }
      });
    });
  </script>
</body>
</html>
