---
title: "Capstone (Advanced): Multistage Policy with SDDP (state, noise, and cuts)"
date: "2026-02-20"
week: 8
lesson: 5
slug: "capstone-advanced-multistage-policy-with-sddp-state-noise-and-cuts"
---

# Topic: Capstone (Advanced): Multistage Policy with SDDP (state, noise, and cuts)

## 1) Formal definition (what is it, and how can we use it?)

This topic delves into a sophisticated application of Stochastic Dual Dynamic Programming (SDDP) to solve multistage stochastic optimization problems, focusing on defining policies based on *state variables*, *noise (uncertainty)*, and *cuts (value function approximations)*. Let's break down the components:

*   **Multistage Stochastic Optimization:** We're dealing with problems where decisions are made sequentially over time, and each decision must consider future uncertainties. The objective is typically to minimize expected costs or maximize expected rewards over the entire planning horizon.

*   **Policy:** A policy maps the current *state* of the system and the realization of the *noise* to a specific decision.  Crucially, in SDDP, the policy is *implicit*, embedded within the Bellman equation and approximated through the cuts. The policy is *not* explicitly defined as a function. Instead, by solving the subproblems at each stage given the cuts approximating the value function, we implicitly determine the optimal decision for a given state and noise realization.

*   **State Variables:** These represent the relevant information about the system at each stage, influencing future decisions and outcomes. Examples include inventory levels, reservoir water levels, or capital investments.  The state variable carries forward information from one stage to the next. The current state is an input to your decision (control) problem for a given stage.

*   **Noise (Uncertainty):** This represents the random variables that affect the system's evolution and costs at each stage. Examples include uncertain demand, weather conditions, or market prices. The noise introduces stochasticity into the optimization. The realization of the noise informs your decision given the current state.

*   **SDDP:**  SDDP is a backward recursion algorithm used to solve multistage stochastic optimization problems. It constructs piecewise linear approximations (cuts) of the value functions (optimal cost-to-go) for each stage.  These cuts are derived from solving forward and backward passes to generate scenarios and then deriving cuts at each stage.

*   **Cuts:**  Cuts represent a piecewise linear approximation of the value function (the optimal cost-to-go from a given state). They are generated by solving forward pass scenarios and backward pass (optimality cut generation).  Each cut is a linear underestimation of the value function.  The lower the value function is estimated the better since we seek to minimize costs (or maximize benefits).

**How it works:**

1.  **Initialization:**  Start with an initial approximation of the value functions for each stage (usually a weak lower bound, such as zero).
2.  **Forward Pass (Scenario Generation):** Simulate multiple scenarios by drawing realizations of the noise and making optimal decisions at each stage, guided by the current value function approximations (cuts). These are simulated forward to estimate the outcomes under a specific policy.
3.  **Backward Pass (Cut Generation):** Starting from the final stage, use the forward pass data to generate cuts that improve the value function approximations.  For each scenario and stage, solve a dual problem to obtain a subgradient of the value function. This subgradient defines a linear cut that supports the value function from below. The cuts are created using the dual variables (Lagrange multipliers) from the subproblems solved during the backward pass.
4.  **Iteration:** Repeat the forward and backward passes until the value function approximations converge. Convergence is typically measured by monitoring the stability of the expected cost in the forward pass.
5.  **Policy Extraction (Implicit):**  While SDDP doesn't explicitly give you a formula for the policy, it *implicitly* defines it.  When a new instance of the problem arises, you use the computed cuts to solve a deterministic equivalent problem stage-by-stage, given the observed state and noise realization. The solution to each stage subproblem reveals the optimal decision for that stage, implicitly implementing the policy.

**Uses:**

This advanced technique is used to solve large-scale, complex problems that cannot be solved by traditional deterministic optimization methods. The policy approach enables more robust and adaptable decision-making in the face of uncertainty.

## 2) Application scenario

**Hydrothermal Scheduling with Reservoir Management**

Consider a power generation company that manages a system of hydroelectric reservoirs and thermal power plants. The company needs to decide how much water to release from the reservoirs for power generation and how much to produce from the thermal plants to meet electricity demand over a multi-period horizon (e.g., weeks or months).

*   **State Variables:**  Reservoir water levels at the beginning of each period.
*   **Noise:** Inflows to the reservoirs (due to rainfall, snowmelt, etc.), electricity demand.
*   **Decisions:** Water release from reservoirs, thermal power generation levels.
*   **Objective:** Minimize the total operating cost, including fuel costs for thermal plants and penalties for unmet demand.
*   **Constraints:** Reservoir capacity constraints, minimum water level requirements, generation capacity limits, water balance equations.

In this scenario, the inflow to the reservoirs and the electricity demand are uncertain. SDDP can be used to find a policy that specifies how much water to release and how much thermal power to generate at each stage, given the current reservoir levels and the realized inflows and demand. The cuts approximate the future cost associated with different reservoir levels, guiding the current decisions.  The SDDP solution would generate cuts at each stage which allow for efficient "look ahead" when making decisions, rather than making decisions in a myopic fashion.

## 3) Python method (if possible)

Several Python libraries facilitate SDDP implementation, particularly those that interface with optimization solvers (like Gurobi or CPLEX).  Here's a simplified example using the `Pyomo` modeling language and a conceptual (not fully executable without a solver configuration) SDDP setup. Note that a full, robust SDDP implementation requires significant effort and careful tuning for specific problems.

```python
import pyomo.environ as pyo
import numpy as np
import random

# --- Simplified Example (Conceptual) ---

def create_stage_model(stage, state_vars, noise_vars, cuts):
    """Creates the Pyomo model for a single stage."""
    model = pyo.ConcreteModel(name=f"Stage_{stage}")

    # State variables (incoming)
    for var_name in state_vars:
        setattr(model, var_name, pyo.Var(domain=pyo.NonNegativeReals, name=f"{var_name}_{stage}")) # Create model.state_var

    # Decision variables (outgoing)
    model.x = pyo.Var(domain=pyo.NonNegativeReals, name=f"x_{stage}") # example control variable

    # Noise variables
    for var_name in noise_vars:
         setattr(model, var_name, pyo.Param(name=f"{var_name}_{stage}", mutable=True)) # mutable allows the value to change during forward and backward passes

    # Cost function (example)
    model.cost = pyo.Objective(expr= model.x**2 +  sum(getattr(model, var_name) for var_name in state_vars) )

    # Transition equations (example: state updates based on decision & noise)
    def _state_transition(model):
        return getattr(model, state_vars[0]) + model.x + getattr(model, noise_vars[0]) >= 0 # Example
    model.state_transition = pyo.Constraint(rule=_state_transition)

    # Value function cuts (add constraints representing the cuts)
    model.cuts = pyo.ConstraintList()
    for cut in cuts:
        def _cut_rule(m):
          return cut['intercept'] + sum(cut['slopes'][i] * getattr(model, state_vars[i]) for i in range(len(state_vars))) <= 0  #The value function
        model.cuts.add(expr = _cut_rule(model))

    return model

def solve_stage_model(model, solver="gurobi"): # needs a real solver defined
    """Solves the stage model and returns the optimal cost and dual variables."""
    opt = pyo.SolverFactory(solver)
    results = opt.solve(model)
    if results.solver.termination_condition != pyo.TerminationCondition.optimal:
      print(f"WARNING: Stage solution not optimal: {results.solver.termination_condition}")

    dual_vars = {}
    for con_name in model.component_objects(pyo.Constraint, active=True):
      constraint = getattr(model, str(con_name))
      for index in constraint:
        dual_vars[str(con_name)] = model.dual[constraint[index]]
    return pyo.value(model.cost), dual_vars

def generate_cut(stage, state_vars, dual_vars, noise_realization, model):
    """Generates a new cut based on the dual variables and state variables."""
    intercept = pyo.value(model.cost)
    slopes = {}
    #For each state variable compute the subgradient
    for var in state_vars:
      if "state_transition" in dual_vars:
        if dual_vars["state_transition"] is not None: #Safety check
            slopes[var] = dual_vars["state_transition"]
        else:
            slopes[var] = 0 # if dual variable unavailable

    cut = {'intercept': intercept, 'slopes': slopes}
    return cut


def sddp_algorithm(num_stages, state_vars, noise_vars, noise_distributions, num_scenarios, solver="gurobi"):
    """Implements the SDDP algorithm."""

    cuts = [[] for _ in range(num_stages)]  # Initialize cuts for each stage

    for iteration in range(10):  # Example: 10 iterations
        print(f"Iteration {iteration}")

        # Forward Pass (Scenario Generation)
        scenario_costs = []
        for scenario in range(num_scenarios):
            print(f"  Scenario {scenario}")
            scenario_cost = 0
            state = {var: np.random.rand() for var in state_vars} #initial states must be provided

            for stage in range(num_stages):
                # Sample noise
                noise = {var: noise_distributions[var]() for var in noise_vars} #generate random noise realizations

                # Create stage model
                model = create_stage_model(stage, state_vars, noise_vars, cuts[stage])

                # Set state and noise values
                for var in state_vars:
                    model_var = getattr(model, var)
                    model_var.fix(state[var]) #Fix the state variables

                for var in noise_vars:
                    model_var = getattr(model, var)
                    model_var.value = noise[var]

                # Solve stage model
                cost, dual_vars = solve_stage_model(model, solver)
                scenario_cost += cost

                # Update state for next stage
                new_state = {}
                for var in state_vars:
                  # Example (using the stage model's solution):
                  new_state[var] = pyo.value(getattr(model, var)) + pyo.value(model.x) + noise[noise_vars[0]]
                state = new_state

            scenario_costs.append(scenario_cost)

        # Backward Pass (Cut Generation)
        for scenario in range(num_scenarios):
          print(f"Backward scenario {scenario}")
          state = {var: np.random.rand() for var in state_vars} # Initial state, needs to be set at initial stage
          for stage in reversed(range(num_stages)):
              # Sample noise
              noise = {var: noise_distributions[var]() for var in noise_vars}
              # Create stage model (same as in forward pass)
              model = create_stage_model(stage, state_vars, noise_vars, cuts[stage])
              #Set initial states
              for var in state_vars:
                  model_var = getattr(model, var)
                  model_var.fix(state[var]) #Fix the state variables

              # Set noise values
              for var in noise_vars:
                  model_var = getattr(model, var)
                  model_var.value = noise[var]
              # Solve stage model
              cost, dual_vars = solve_stage_model(model, solver)

              # Generate cut
              cut = generate_cut(stage, state_vars, dual_vars, noise, model)
              cuts[stage].append(cut)

              # Update state based on transition (using the *optimal* decision from solving the stage model)
              new_state = {}
              for var in state_vars:
                  # Example (using the stage model's solution):
                  new_state[var] = pyo.value(getattr(model, var)) + pyo.value(model.x) + noise[noise_vars[0]]
              state = new_state
              #Remove all cuts at end of backward pass
              for model_stage in cuts:
                  model_stage.clear()


        avg_cost = np.mean(scenario_costs)
        print(f"  Average Scenario Cost: {avg_cost}")

    return cuts #The cuts approximate the value function

# --- Example Usage ---

num_stages = 3
state_vars = ["inventory"] # one state variable
noise_vars = ["demand"] # one noise variable
noise_distributions = {"demand": lambda: random.uniform(0, 10)} # Example demand distribution
num_scenarios = 5

# Run SDDP
cuts = sddp_algorithm(num_stages, state_vars, noise_vars, noise_distributions, num_scenarios)

print("SDDP complete.  Value function approximations (cuts) generated.")
```

**Explanation:**

*   **`create_stage_model`:** Defines the Pyomo model for a single stage.  Includes state variables, decision variables, noise variables, the cost function, constraints (including state transition equations), and incorporates the value function approximation (cuts).
*   **`solve_stage_model`:** Solves the stage model using a specified solver (e.g., Gurobi).  Returns the optimal cost and the dual variables associated with the constraints.
*   **`generate_cut`:**  Creates a new cut based on the dual variables obtained from solving the stage model. This leverages the fact that dual variables provide subgradients of the value function.
*   **`sddp_algorithm`:** Implements the main SDDP loop, performing forward and backward passes.  It iteratively refines the value function approximations (cuts) until convergence.  The forward pass simulates scenarios, and the backward pass generates cuts based on the dual variables.
*   **`noise_distributions`:**  A dictionary holding functions to sample from the distributions of the noise variables.
*   **Key Pyomo Concepts:**  `pyo.ConcreteModel`, `pyo.Var`, `pyo.Constraint`, `pyo.Objective`, `pyo.SolverFactory`, `model.dual` (accessing dual variables).

**Important Considerations:**

*   **Solver:** You'll need a suitable optimization solver installed (Gurobi, CPLEX, etc.) and properly configured with Pyomo.
*   **Convergence:**  The code includes a fixed number of iterations.  In a real application, you would implement a convergence criterion based on the stability of the objective function value or the change in the cuts.
*   **Scenario Generation:** The number of scenarios and the way they are generated are critical for the accuracy and convergence of SDDP.  Variance reduction techniques (e.g., Latin Hypercube Sampling) can improve efficiency.
*   **Problem-Specific Modeling:** The model structure, cost function, constraints, and state transition equations are highly problem-specific. You need to carefully formulate these based on the application.
*   **Cut Management:**  Over time, the number of cuts can grow significantly. Strategies for cut selection and deletion are often needed to manage the size of the model.

## 4) Follow-up question

How do variance reduction techniques, such as Latin Hypercube Sampling, specifically impact the convergence speed and accuracy of the SDDP algorithm, and are there any drawbacks to using these techniques in practice?