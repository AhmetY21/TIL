<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Visualizing Embeddings (t-SNE, PCA)</title>
  <script>
    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
  </script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }

    .page-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 24px;
      padding-bottom: 16px;
      border-bottom: 1px solid #e5e7eb;
    }
    .back-link {
      font-weight: 600;
      text-decoration: none;
    }
    .back-link:hover {
      text-decoration: underline;
    }

    .theme-toggle {
      background: none;
      border: none;
      cursor: pointer;
      font-size: 1.5rem;
      padding: 8px;
      border-radius: 50%;
      transition: background 0.2s;
    }
    .theme-toggle:hover {
      background: rgba(0,0,0,0.05);
    }
    .dark .theme-toggle:hover {
      background: rgba(255,255,255,0.1);
    }

    .dark body {
      background: #0f172a;
      color: #e2e8f0;
    }
    .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
    .dark a { color: #60a5fa; }
    .dark .page-header { border-bottom-color: #334155; }
    .dark code { background-color: #1e293b; color: #e2e8f0; }
    .dark pre {
      border: 1px solid #334155;
    }
    .dark blockquote {
      border-left-color: #334155;
      color: #94a3b8;
    }
    .dark th, .dark td { border-color: #334155; }
    .dark th { background: #1e293b; }
    .dark hr { border-top-color: #334155; }

    /* Copy Button */
    pre { position: relative; }
    .copy-button {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      font-size: 0.8rem;
      color: #94a3b8;
      background: rgba(255, 255, 255, 0.1);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 4px;
      cursor: pointer;
      opacity: 0;
      transition: opacity 0.2s, background 0.2s;
    }
    pre:hover .copy-button, .copy-button:focus {
      opacity: 1;
    }
    .copy-button:hover {
      background: rgba(255, 255, 255, 0.2);
      color: #e2e8f0;
    }

    .skip-link {
      position: absolute;
      top: -40px;
      left: 0;
      background: #2563eb;
      color: white;
      padding: 8px 16px;
      z-index: 100;
      transition: top 0.2s;
      text-decoration: none;
      font-weight: bold;
    }
    .skip-link:focus {
      top: 0;
    }

  </style>
</head>
<body>
  <a href="#main-content" class="skip-link">Skip to content</a>

  <div class="page-header">
    <a href="../../../../../hubs/nlp-index.html" class="back-link">‚Üê Back to Natural Language Processing</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle Dark Mode">üåô</button>
  </div>
  <main id="main-content" tabindex="-1">

<h1 id="topic-visualizing-embeddings-t-sne-pca">Topic: Visualizing Embeddings (t-SNE, PCA)</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>Visualizing embeddings, particularly using techniques like t-distributed Stochastic Neighbor Embedding (t-SNE) and Principal Component Analysis (PCA), is a crucial process in NLP for understanding the structure and relationships within high-dimensional vector representations of words, phrases, sentences, or even documents. These embeddings are generated by various NLP models like Word2Vec, GloVe, BERT, and others. Due to their high dimensionality (often hundreds or thousands of dimensions), it is difficult to directly interpret or analyze them. Dimensionality reduction techniques are used to project these high-dimensional embeddings onto a lower-dimensional space (typically 2D or 3D) that can be visualized on a scatter plot.</p>
<p><strong>PCA (Principal Component Analysis):</strong> A linear dimensionality reduction technique that identifies orthogonal principal components (axes) in the data that capture the maximum variance. It projects the data onto these components, effectively reducing the number of dimensions while retaining as much information as possible about the original data's variance. PCA is relatively fast and simple but can sometimes fail to capture complex non-linear relationships.</p>
<p><strong>t-SNE (t-distributed Stochastic Neighbor Embedding):</strong> A non-linear dimensionality reduction technique that aims to preserve the local structure of the data. It models the probability distribution of neighbors around each point in both the high-dimensional space and the low-dimensional space, then minimizes the Kullback-Leibler (KL) divergence between these distributions. t-SNE excels at revealing clusters in the data but is computationally more expensive than PCA and sensitive to parameter tuning.  It's mainly used for visualization and not for feature engineering.</p>
<p><strong>How can we use it?</strong></p>
<ul>
<li><strong>Understanding Word Semantics:</strong> By visualizing word embeddings, we can observe which words are clustered together, revealing semantic relationships. For instance, "king," "queen," "prince," and "princess" might form a cluster, illustrating that they share a similar meaning related to royalty.</li>
<li><strong>Evaluating Embedding Quality:</strong> Visualizations can help assess the quality of embeddings. A well-trained embedding space should show meaningful clusters reflecting the underlying data structure. Overlapping or poorly separated clusters might indicate problems with the training data or model.</li>
<li><strong>Identifying Biases:</strong> Visualizing embeddings can uncover biases encoded in the data or the model. For example, gender stereotypes may become apparent if words associated with certain professions are clustered with words associated with a specific gender.</li>
<li><strong>Document/Sentence Similarity:</strong> Sentence embeddings can be reduced and visualized to understand relationships between sentences or documents. Clustering will show groups of sentences with similar meaning or topic.</li>
<li><strong>Model Debugging:</strong> Can help identify anomalies and outliers in the embedded data, which can then guide investigation into the model.</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p><strong>Scenario:</strong> Imagine you have trained a Word2Vec model on a large corpus of news articles related to finance. You want to understand how different financial terms are related to each other according to the model.</p>
<p><strong>Steps:</strong></p>
<ol>
<li><strong>Generate Word Embeddings:</strong> Use your trained Word2Vec model to obtain word embeddings for relevant financial terms like "inflation," "interest rate," "stock market," "bond," "GDP," "unemployment," etc.</li>
<li><strong>Dimensionality Reduction:</strong> Apply t-SNE to reduce the high-dimensional word embeddings (e.g., 100 dimensions) to 2 or 3 dimensions. This allows you to plot the words on a scatter plot.</li>
<li><strong>Visualization:</strong> Create a scatter plot where each point represents a word. Use different colors or shapes to highlight specific categories of words (e.g., macroeconomic indicators, investment instruments).</li>
<li><strong>Interpretation:</strong> Analyze the plot to identify clusters of related words. For example, you might observe that "inflation" and "interest rate" are clustered together, reflecting their close relationship in economics.  Similarly, "stock market" and "bond" might form another cluster representing investment options. You could also discover unexpected relationships, such as "GDP" being closer to "unemployment" than expected, indicating a model focus on their inverse relationship.</li>
</ol>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<pre class="codehilite"><code class="language-python">import numpy as np
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Sample data (replace with your actual word embeddings)
# This is a dictionary where keys are words and values are their embeddings.
# In a real scenario this would come from your trained word2vec model.
embeddings = {
    &quot;king&quot;: np.array([0.1, 0.2, 0.3, 0.4, 0.5]),
    &quot;queen&quot;: np.array([0.15, 0.25, 0.35, 0.45, 0.55]),
    &quot;man&quot;: np.array([0.6, 0.7, 0.8, 0.9, 1.0]),
    &quot;woman&quot;: np.array([0.65, 0.75, 0.85, 0.95, 1.05]),
    &quot;apple&quot;: np.array([0.2, 0.8, 0.1, 0.9, 0.4]),
    &quot;banana&quot;: np.array([0.25, 0.75, 0.15, 0.85, 0.45])
}

words = list(embeddings.keys())
embedding_vectors = np.array(list(embeddings.values()))

# --- Using t-SNE ---
tsne = TSNE(n_components=2, perplexity=3, random_state=42)  # Adjust parameters as needed
reduced_embeddings_tsne = tsne.fit_transform(embedding_vectors)

# Plotting t-SNE results
plt.figure(figsize=(8, 6))
for i, word in enumerate(words):
    plt.scatter(reduced_embeddings_tsne[i, 0], reduced_embeddings_tsne[i, 1], marker='o', color='blue')
    plt.annotate(word, xy=(reduced_embeddings_tsne[i, 0], reduced_embeddings_tsne[i, 1]), xytext=(5, 2),
                 textcoords='offset points', ha='right', va='bottom')
plt.title(&quot;t-SNE Visualization of Word Embeddings&quot;)
plt.xlabel(&quot;Dimension 1&quot;)
plt.ylabel(&quot;Dimension 2&quot;)
plt.show()

# --- Using PCA ---
pca = PCA(n_components=2)
reduced_embeddings_pca = pca.fit_transform(embedding_vectors)

# Plotting PCA results
plt.figure(figsize=(8, 6))
for i, word in enumerate(words):
    plt.scatter(reduced_embeddings_pca[i, 0], reduced_embeddings_pca[i, 1], marker='o', color='red')
    plt.annotate(word, xy=(reduced_embeddings_pca[i, 0], reduced_embeddings_pca[i, 1]), xytext=(5, 2),
                 textcoords='offset points', ha='right', va='bottom')
plt.title(&quot;PCA Visualization of Word Embeddings&quot;)
plt.xlabel(&quot;Principal Component 1&quot;)
plt.ylabel(&quot;Principal Component 2&quot;)
plt.show()
</code></pre>

<p><strong>Explanation:</strong></p>
<ol>
<li><strong>Import Libraries:</strong> Import necessary libraries like <code>numpy</code> for numerical operations, <code>TSNE</code> and <code>PCA</code> from <code>sklearn.manifold</code> and <code>sklearn.decomposition</code> respectively for dimensionality reduction, and <code>matplotlib</code> for plotting.</li>
<li><strong>Prepare Data:</strong> The code starts with a sample dictionary called <code>embeddings</code>.  In a real-world scenario, this would be populated with the output vectors from the chosen embedding model (like Word2Vec, GloVe, BERT, etc.). The sample contains simple vectors for a few common words to make the visualization code runnable. We convert the embeddings into a NumPy array for processing.</li>
<li><strong>t-SNE:</strong> The code then initializes a <code>TSNE</code> object with the desired number of components (2 for 2D visualization) and a <code>perplexity</code> parameter. The <code>perplexity</code> is a crucial parameter in t-SNE which relates to the number of nearest neighbors used in the algorithm. A suitable value for perplexity can be between 5 and 50. The <code>fit_transform</code> method applies t-SNE to the embedding vectors and reduces them to 2 dimensions.</li>
<li><strong>PCA:</strong> An instance of PCA is created and applied to reduce the embedding vectors.</li>
<li><strong>Plotting:</strong> The code uses <code>matplotlib</code> to create a scatter plot of the reduced embeddings. Each word is represented as a point on the plot, and the word label is added as an annotation.</li>
</ol>
<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>How can we quantitatively evaluate the quality of a word embedding space, aside from qualitative visualization? Are there any metrics that can tell us if one set of embeddings is "better" than another, and what are some of the limitations of these evaluation metrics?</p>
  </main>
  <script>
    const btn = document.getElementById('theme-toggle');
    const html = document.documentElement;
    
    function updateIcon() {
      btn.textContent = html.classList.contains('dark') ? '‚òÄÔ∏è' : 'üåô';
    }
    
    // Set initial icon
    updateIcon();

    btn.addEventListener('click', () => {
      if (html.classList.contains('dark')) {
        html.classList.remove('dark');
        localStorage.theme = 'light';
      } else {
        html.classList.add('dark');
        localStorage.theme = 'dark';
      }
      updateIcon();
    });

    // Copy Code Functionality
    document.querySelectorAll('pre').forEach(pre => {
      const button = document.createElement('button');
      button.className = 'copy-button';
      button.textContent = 'Copy';
      button.setAttribute('aria-label', 'Copy code to clipboard');

      pre.appendChild(button);

      button.addEventListener('click', async () => {
        const code = pre.querySelector('code');
        if (!code) return;

        try {
          await navigator.clipboard.writeText(code.innerText);
          button.textContent = 'Copied!';
          setTimeout(() => {
            button.textContent = 'Copy';
          }, 2000);
        } catch (err) {
          console.error('Failed to copy:', err);
          button.textContent = 'Error';
        }
      });
    });
  </script>
</body>
</html>
