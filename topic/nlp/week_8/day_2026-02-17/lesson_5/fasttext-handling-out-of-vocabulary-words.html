<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FastText: Handling Out-of-Vocabulary Words</title>
  <script>
    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
  </script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }

    .page-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 24px;
      padding-bottom: 16px;
      border-bottom: 1px solid #e5e7eb;
    }
    .back-link {
      font-weight: 600;
      text-decoration: none;
    }
    .back-link:hover {
      text-decoration: underline;
    }

    .theme-toggle {
      background: none;
      border: none;
      cursor: pointer;
      font-size: 1.5rem;
      padding: 8px;
      border-radius: 50%;
      transition: background 0.2s;
    }
    .theme-toggle:hover {
      background: rgba(0,0,0,0.05);
    }
    .dark .theme-toggle:hover {
      background: rgba(255,255,255,0.1);
    }

    .dark body {
      background: #0f172a;
      color: #e2e8f0;
    }
    .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
    .dark a { color: #60a5fa; }
    .dark .page-header { border-bottom-color: #334155; }
    .dark code { background-color: #1e293b; color: #e2e8f0; }
    .dark pre {
      border: 1px solid #334155;
    }
    .dark blockquote {
      border-left-color: #334155;
      color: #94a3b8;
    }
    .dark th, .dark td { border-color: #334155; }
    .dark th { background: #1e293b; }
    .dark hr { border-top-color: #334155; }

    /* Copy Button */
    pre { position: relative; }
    .copy-button {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      font-size: 0.8rem;
      color: #94a3b8;
      background: rgba(255, 255, 255, 0.1);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 4px;
      cursor: pointer;
      opacity: 0;
      transition: opacity 0.2s, background 0.2s;
    }
    pre:hover .copy-button, .copy-button:focus {
      opacity: 1;
    }
    .copy-button:hover {
      background: rgba(255, 255, 255, 0.2);
      color: #e2e8f0;
    }
  </style>
</head>
<body>

  <div class="page-header">
    <a href="../../../../../hubs/nlp-index.html" class="back-link">‚Üê Back to Natural Language Processing</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle Dark Mode">üåô</button>
  </div>

<h1 id="topic-fasttext-handling-out-of-vocabulary-words">Topic: FastText: Handling Out-of-Vocabulary Words</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>FastText is a word embedding technique developed by Facebook AI Research. While sharing similarities with Word2Vec (specifically the Skip-gram and CBOW models), FastText's key innovation is its ability to handle out-of-vocabulary (OOV) words. This is achieved by representing each word as the sum of character n-grams.</p>
<p>Specifically, instead of learning a single vector representation for each word in the vocabulary, FastText decomposes words into character n-grams. For example, consider the word "eating" and suppose we're using n=3 (trigrams).  FastText would represent "eating" as the collection of these trigrams:</p>
<p><code>&lt;ea</code>, <code>eat</code>, <code>ati</code>, <code>tin</code>, <code>ing</code>, <code>ng&gt;</code> and the special character <code>&lt;</code> and <code>&gt;</code> denoting the start and end of the word (i.e., <code>&lt;eating&gt;</code>). It also includes the whole word <code>&lt;eating&gt;</code> as a feature.</p>
<p>When encountering an OOV word, FastText can still generate a word embedding by summing the embeddings of its constituent n-grams.  Even if the entire word has never been seen before, some of its n-grams likely have.  This allows FastText to produce a reasonable vector representation for unseen words, making it significantly more robust than methods like Word2Vec, which simply assign a random vector or an "UNK" (unknown) token to OOV words.</p>
<p>We can use FastText to:</p>
<ul>
<li><strong>Generate word embeddings:</strong> Like Word2Vec, it produces vector representations of words that capture semantic relationships.</li>
<li><strong>Handle OOV words:</strong> This is its primary advantage, allowing it to process text with unseen words effectively.</li>
<li><strong>Text classification:</strong> The learned word embeddings can be used as features for various text classification tasks.</li>
<li><strong>Word similarity tasks:</strong> Determining how similar words are to each other based on their embeddings.</li>
<li><strong>Language identification:</strong>  Using character n-grams can be helpful for identifying the language of a text, even with limited data.</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Consider a situation where you are building a sentiment analysis model for a social media platform. This platform is constantly evolving, and new slang terms and abbreviations are being created all the time. A standard Word2Vec model would struggle with these new, unseen words, potentially leading to poor performance.</p>
<p>Using FastText in this scenario would be beneficial. Even if a new slang term like "yeet" appears, FastText can generate a reasonable embedding for it based on its constituent character n-grams. This allows the sentiment analysis model to handle new vocabulary more gracefully and maintain accuracy even when faced with unseen words.</p>
<p>Another scenario is in multilingual text processing.  Even if your training data doesn't contain words from a specific language, if that language shares similar characters and n-gram patterns with languages in your training data, FastText can still generate somewhat meaningful embeddings.</p>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<p>You can use the <code>fasttext</code> library in Python to train and use FastText models. Here's a code example demonstrating how to train a model and get the vector for an OOV word:</p>
<pre class="codehilite"><code class="language-python">import fasttext

# Sample training data (replace with your actual data file)
with open(&quot;training_data.txt&quot;, &quot;w&quot;) as f:
    f.write(&quot;This is a sample sentence.\n&quot;)
    f.write(&quot;Another sentence for training.\n&quot;)
    f.write(&quot;FastText is awesome!\n&quot;)

# Train a FastText model (skipgram architecture)
model = fasttext.train_unsupervised('training_data.txt', model='skipgram')

# Get the vector for a known word
vector_known = model.get_word_vector(&quot;awesome&quot;)
print(&quot;Vector for 'awesome':&quot;, vector_known[:5]) # print first 5 dimensions

# Get the vector for an out-of-vocabulary word
vector_oov = model.get_word_vector(&quot;unseenword&quot;)
print(&quot;Vector for 'unseenword':&quot;, vector_oov[:5]) # print first 5 dimensions

# Save the model
model.save_model(&quot;fasttext_model.bin&quot;)

# Load the model (optional)
loaded_model = fasttext.load_model(&quot;fasttext_model.bin&quot;)

# Clean up training data
import os
os.remove(&quot;training_data.txt&quot;)
os.remove(&quot;fasttext_model.bin&quot;)
</code></pre>

<p><strong>Explanation:</strong></p>
<ol>
<li><strong>Import <code>fasttext</code>:</strong> Imports the necessary library.</li>
<li><strong>Training Data:</strong> Creates a simple training data file ( <code>training_data.txt</code>). <strong>Important:</strong> Replace this with your actual training data. FastText expects a text file where each line is a sentence.</li>
<li><strong><code>fasttext.train_unsupervised()</code>:</strong> Trains a FastText model in unsupervised mode.<ul>
<li><code>'training_data.txt'</code> specifies the path to the training data.</li>
<li><code>model='skipgram'</code> selects the Skip-gram architecture (alternatively, you can use <code>'cbow'</code>).  Other parameters like <code>lr</code> (learning rate), <code>dim</code> (embedding dimension), <code>epoch</code> (number of training epochs), and <code>minCount</code> (minimum word count) can also be specified.</li>
</ul>
</li>
<li><strong><code>model.get_word_vector(word)</code>:</strong>  Retrieves the word vector for the given word.  The example shows how to get vectors for both a known word ("awesome") and an OOV word ("unseenword").</li>
<li><strong><code>model.save_model(filename)</code>:</strong> Saves the trained model to a file.</li>
<li><strong><code>fasttext.load_model(filename)</code>:</strong> Loads a previously saved model.</li>
<li><strong>Cleanup:</strong> Delete the training file and the trained model to avoid clutter.</li>
</ol>
<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>How does the choice of the n-gram size (the 'n' in n-grams) affect the performance of FastText, particularly regarding handling OOV words and computational efficiency? What are the trade-offs involved in selecting different n-gram sizes?</p>
  <script>
    const btn = document.getElementById('theme-toggle');
    const html = document.documentElement;
    
    function updateIcon() {
      btn.textContent = html.classList.contains('dark') ? '‚òÄÔ∏è' : 'üåô';
    }
    
    // Set initial icon
    updateIcon();

    btn.addEventListener('click', () => {
      if (html.classList.contains('dark')) {
        html.classList.remove('dark');
        localStorage.theme = 'light';
      } else {
        html.classList.add('dark');
        localStorage.theme = 'dark';
      }
      updateIcon();
    });

    // Copy Code Functionality
    document.querySelectorAll('pre').forEach(pre => {
      const button = document.createElement('button');
      button.className = 'copy-button';
      button.textContent = 'Copy';
      button.setAttribute('aria-label', 'Copy code to clipboard');

      pre.appendChild(button);

      button.addEventListener('click', async () => {
        const code = pre.querySelector('code');
        if (!code) return;

        try {
          await navigator.clipboard.writeText(code.innerText);
          button.textContent = 'Copied!';
          setTimeout(() => {
            button.textContent = 'Copy';
          }, 2000);
        } catch (err) {
          console.error('Failed to copy:', err);
          button.textContent = 'Error';
        }
      });
    });
  </script>
</body>
</html>
