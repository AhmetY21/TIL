<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Machine Translation with Seq2Seq</title>
  <script>
    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
  </script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }

    .page-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 24px;
      padding-bottom: 16px;
      border-bottom: 1px solid #e5e7eb;
    }
    .back-link {
      font-weight: 600;
      text-decoration: none;
    }
    .back-link:hover {
      text-decoration: underline;
    }

    .theme-toggle {
      background: none;
      border: none;
      cursor: pointer;
      font-size: 1.5rem;
      padding: 8px;
      border-radius: 50%;
      transition: background 0.2s;
    }
    .theme-toggle:hover {
      background: rgba(0,0,0,0.05);
    }
    .dark .theme-toggle:hover {
      background: rgba(255,255,255,0.1);
    }

    .dark body {
      background: #0f172a;
      color: #e2e8f0;
    }
    .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
    .dark a { color: #60a5fa; }
    .dark .page-header { border-bottom-color: #334155; }
    .dark code { background-color: #1e293b; color: #e2e8f0; }
    .dark pre {
      border: 1px solid #334155;
    }
    .dark blockquote {
      border-left-color: #334155;
      color: #94a3b8;
    }
    .dark th, .dark td { border-color: #334155; }
    .dark th { background: #1e293b; }
    .dark hr { border-top-color: #334155; }

    /* Copy Button */
    pre { position: relative; }
    .copy-button {
      position: absolute;
      top: 8px;
      right: 8px;
      padding: 4px 8px;
      font-size: 0.8rem;
      color: #94a3b8;
      background: rgba(255, 255, 255, 0.1);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 4px;
      cursor: pointer;
      opacity: 0;
      transition: opacity 0.2s, background 0.2s;
    }
    pre:hover .copy-button, .copy-button:focus {
      opacity: 1;
    }
    .copy-button:hover {
      background: rgba(255, 255, 255, 0.2);
      color: #e2e8f0;
    }

    /* Skip Link */
    .skip-link {
      position: absolute;
      top: -40px;
      left: 0;
      background: #0f172a;
      color: white;
      padding: 8px 16px;
      z-index: 100;
      transition: top 0.2s;
      font-weight: 600;
      border-bottom-right-radius: 6px;
    }
    .skip-link:focus {
      top: 0;
    }

  </style>
</head>
<body>
  <a href="#main-content" class="skip-link">Skip to content</a>

  <div class="page-header">
    <a href="../../../../../hubs/nlp-index.html" class="back-link">‚Üê Back to Natural Language Processing</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle Dark Mode">üåô</button>
  </div>
  <main id="main-content">

<h1 id="topic-machine-translation-with-seq2seq">Topic: Machine Translation with Seq2Seq</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>Machine Translation (MT) with Seq2Seq (Sequence-to-Sequence) is a neural network architecture designed to map an input sequence (e.g., a sentence in English) to an output sequence (e.g., the same sentence in French). The "sequence" aspect is crucial as it handles the variable-length nature of sentences.</p>
<p>Formally, a Seq2Seq model typically consists of two main components:</p>
<ul>
<li>
<p><strong>Encoder:</strong> The encoder takes the input sequence as input and compresses it into a fixed-length vector representation called the "context vector" or "thought vector".  This vector aims to encapsulate the semantic meaning of the entire input sequence.  It's often implemented as a Recurrent Neural Network (RNN), such as an LSTM or GRU, processing the input sequence word by word.  The final hidden state of the RNN encoder becomes the context vector.</p>
</li>
<li>
<p><strong>Decoder:</strong> The decoder takes the context vector produced by the encoder and generates the output sequence word by word. It's also usually an RNN (LSTM or GRU) initialized with the context vector as its initial hidden state.  At each time step, the decoder predicts the next word in the output sequence, conditioned on the context vector and the previously generated words.</p>
</li>
</ul>
<p>During training, the model learns to map input sequences to their corresponding output sequences. This is typically done using a parallel corpus, which contains pairs of sentences in the source and target languages that are translations of each other. The model is trained to minimize a loss function, such as cross-entropy, which measures the difference between the predicted output sequence and the actual target sequence.</p>
<p>We can use Seq2Seq models for various tasks, including:</p>
<ul>
<li><strong>Machine Translation:</strong> Translating text from one language to another.</li>
<li><strong>Text Summarization:</strong> Condensing a long piece of text into a shorter summary.</li>
<li><strong>Chatbots:</strong> Generating responses to user input.</li>
<li><strong>Code Generation:</strong>  Generating code from natural language descriptions.</li>
<li><strong>Image Captioning:</strong>  Generating text descriptions of images (in this case, the encoder would be a CNN extracting features from the image).</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Imagine we want to translate English sentences into German. We can use a Seq2Seq model for this purpose.</p>
<p><strong>Input:</strong> "Hello, how are you?" (English)
<strong>Output:</strong> "Hallo, wie geht es dir?" (German)</p>
<ol>
<li>
<p>The <strong>Encoder</strong> takes the English sentence "Hello, how are you?" as input. It processes the sentence word by word (or subword) using an RNN (e.g., an LSTM). The final hidden state of the LSTM represents the "context" of the English sentence.</p>
</li>
<li>
<p>The <strong>Decoder</strong> receives the context vector from the encoder. It initializes its own LSTM with this context vector. Then, it starts generating the German translation.</p>
</li>
<li>
<p>At the first time step, the decoder might predict "Hallo". This prediction is based on the context vector and a special "start-of-sequence" token.</p>
</li>
<li>
<p>At the second time step, the decoder takes "Hallo" (or its embedding representation) as input, along with the previous hidden state, and predicts "wie".</p>
</li>
<li>
<p>This process continues until the decoder predicts a special "end-of-sequence" token, indicating the end of the German sentence.</p>
</li>
</ol>
<p>The training data would consist of many English-German sentence pairs.  The model learns the statistical relationships between the two languages by minimizing the error between the predicted German sentences and the actual German translations in the training data.  Attention mechanisms (mentioned in the follow-up question) significantly improve the performance of this process, especially for longer sentences.</p>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<p>Here's a simplified example using TensorFlow/Keras to illustrate the basic structure of a Seq2Seq model.  A full implementation for machine translation would be much more complex, involving padding, masking, tokenization, and attention mechanisms. This provides the <em>structure</em> but is not directly runnable for translation without significant addition work to prepare the data.</p>
<pre class="codehilite"><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define model parameters
embedding_dim = 64  # Dimensionality of word embeddings
units = 128  # Number of LSTM units
input_vocab_size = 10000 #Size of English Vocabulary.  Needs to be built from actual data.
output_vocab_size = 8000 #Size of German Vocabulary. Needs to be built from actual data.
max_length = 20 #Maximum length of sentences. Needs to be determined by data analysis.

# Encoder
encoder_inputs = keras.Input(shape=(max_length,), dtype=&quot;int64&quot;)
x = layers.Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
encoder_lstm = layers.LSTM(units, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(x)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = keras.Input(shape=(max_length,), dtype=&quot;int64&quot;)
decoder_embedding = layers.Embedding(output_vocab_size, embedding_dim)
x = decoder_embedding(decoder_inputs)
decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(x, initial_state=encoder_states)
decoder_dense = layers.Dense(output_vocab_size, activation=&quot;softmax&quot;)
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(
    optimizer=&quot;rmsprop&quot;, loss=&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)

# Print model summary
model.summary()

# Example usage (needs preprocessed data - indices of words)
import numpy as np
encoder_input_data = np.random.randint(0, input_vocab_size, size=(100, max_length))
decoder_input_data = np.random.randint(0, output_vocab_size, size=(100, max_length))
decoder_target_data = np.random.randint(0, output_vocab_size, size=(100, max_length)) #shifted decoder_input_data, one time step ahead for training. Crucial!

model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=10)
</code></pre>

<p><strong>Explanation:</strong></p>
<ul>
<li><strong>Encoder:</strong> Takes integer sequences as input (representing word indices).  An embedding layer converts those integer indices to dense vectors. The LSTM processes these embeddings and produces the context vector (hidden states).</li>
<li><strong>Decoder:</strong>  Also takes integer sequences as input. Uses an embedding layer and an LSTM.  Crucially, it's initialized with the encoder's hidden states.  A dense layer with a softmax activation predicts the probability distribution over the output vocabulary.</li>
<li><strong>Training:</strong> During training, <code>decoder_target_data</code> is a one-step-ahead version of <code>decoder_input_data</code>. This is because the decoder is learning to predict the next word given the current word and the context.</li>
</ul>
<p><strong>Important Notes:</strong></p>
<ul>
<li>This is a very basic example and lacks crucial components for real-world translation, such as:<ul>
<li><strong>Data Preprocessing:</strong> Tokenization, vocabulary creation, padding, and masking.</li>
<li><strong>Attention Mechanisms:</strong>  To allow the decoder to focus on different parts of the input sequence when generating each output word (greatly improves translation quality).</li>
<li><strong>Beam Search:</strong>  To improve the decoding process by exploring multiple possible translations.</li>
<li><strong>Subword Tokenization (e.g., BPE):</strong> To handle rare words and improve generalization.</li>
</ul>
</li>
<li>The input and output data are represented as integer indices, assuming you have already built a vocabulary and tokenized your text data.</li>
<li>The <code>sparse_categorical_crossentropy</code> loss function is commonly used when the output labels are integers (word indices).</li>
</ul>
<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>Seq2Seq models are powerful, but the basic encoder-decoder architecture described above suffers from performance degradation when dealing with long sequences. This is because the entire input sequence is compressed into a single fixed-length context vector, which can become a bottleneck for long sentences.</p>
<p>How can we improve the performance of Seq2Seq models for long sequences? Explain the core idea and functionality of the "Attention Mechanism" in the context of Seq2Seq models.</p>
    </main>
<script>
    const btn = document.getElementById('theme-toggle');
    const html = document.documentElement;
    
    function updateIcon() {
      btn.textContent = html.classList.contains('dark') ? '‚òÄÔ∏è' : 'üåô';
    }
    
    // Set initial icon
    updateIcon();

    btn.addEventListener('click', () => {
      if (html.classList.contains('dark')) {
        html.classList.remove('dark');
        localStorage.theme = 'light';
      } else {
        html.classList.add('dark');
        localStorage.theme = 'dark';
      }
      updateIcon();
    });

    // Copy Code Functionality
    document.querySelectorAll('pre').forEach(pre => {
      const button = document.createElement('button');
      button.className = 'copy-button';
      button.textContent = 'Copy';
      button.setAttribute('aria-label', 'Copy code to clipboard');

      pre.appendChild(button);

      button.addEventListener('click', async () => {
        const code = pre.querySelector('code');
        if (!code) return;

        try {
          await navigator.clipboard.writeText(code.innerText);
          button.textContent = 'Copied!';
          setTimeout(() => {
            button.textContent = 'Copy';
          }, 2000);
        } catch (err) {
          console.error('Failed to copy:', err);
          button.textContent = 'Error';
        }
      });
    });
  </script>
</body>
</html>
