---
title: "Encoder-Decoder Architecture"
date: "2026-02-20"
week: 8
lesson: 5
slug: "encoder-decoder-architecture"
---

# Topic: Encoder-Decoder Architecture

## 1) Formal definition (what is it, and how can we use it?)

The Encoder-Decoder architecture is a powerful deep learning framework commonly used in natural language processing and other sequence-to-sequence tasks. It consists of two main components:

*   **Encoder:** The encoder takes a sequence of input data (e.g., a sentence in English) and transforms it into a fixed-length vector representation, often called the "context vector" or "thought vector".  This vector is intended to capture the essence and meaning of the entire input sequence.

*   **Decoder:** The decoder takes the context vector generated by the encoder and uses it to generate a new sequence of output data (e.g., a sentence in French, a code snippet, or an image caption).  It generates the output sequence one element at a time, often conditioned on previously generated elements and the context vector.

**How can we use it?**

The encoder-decoder architecture is useful for tasks where the input and output are both sequences, but the output is not a direct transformation of the input.  Key characteristics include:

*   **Variable Length Input and Output:**  Handles sequences of different lengths for both input and output.
*   **Sequence-to-Sequence Mapping:**  Learns a complex mapping between input and output sequences, rather than simply predicting a single value.
*   **Abstraction of Input Information:** The context vector forces the encoder to compress the entire input sequence into a single representation, allowing the decoder to generate the output without needing to directly refer back to the original input.

Examples of use cases include:

*   **Machine Translation:** Encoding a sentence in one language and decoding it into another.
*   **Text Summarization:** Encoding a long document and decoding a shorter summary.
*   **Image Captioning:** Encoding an image (using a CNN as the encoder) and decoding a textual description.
*   **Speech Recognition:** Encoding an audio waveform and decoding it into text.
*   **Code Generation:** Encoding a natural language description of code and decoding the corresponding code snippet.

## 2) Application scenario

Let's consider the application scenario of **Machine Translation (English to French)**.

1.  **Encoder:** An English sentence like "The cat sat on the mat" is fed into the encoder.  The encoder, typically a recurrent neural network (RNN) like LSTM or GRU, processes the sentence word by word. After processing the entire sentence, the encoder outputs a context vector that represents the meaning of the English sentence.

2.  **Decoder:** The decoder, also an RNN, takes the context vector as its initial hidden state. It then generates the French translation, one word at a time. The decoder might first output "Le", then "chat", then "Ã©tait", then "assis", then "sur", then "le", and finally "tapis".  At each step, the decoder considers the context vector and the previously generated words to predict the next word in the French sentence.

In this scenario, the encoder compresses the meaning of the English sentence into a fixed-length vector, and the decoder uses this vector to generate a grammatically correct and semantically equivalent French sentence. The model learns the complex relationship between the two languages through training on a large parallel corpus of English-French sentence pairs. Attention mechanisms can be added to this architecture to further improve the translation quality, allowing the decoder to focus on specific parts of the input sentence when generating each output word.

## 3) Python method (if possible)

Here's a simplified example of an Encoder-Decoder architecture using TensorFlow/Keras. This is a highly simplified version for illustration; real-world implementations are more complex and often use attention mechanisms.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding

# Define model parameters
embedding_dim = 256  # Dimensionality of word embeddings
latent_dim = 256     # Dimensionality of the LSTM hidden state
num_encoder_tokens = 10000 # Vocabulary size for the encoder (English)
num_decoder_tokens = 10000 # Vocabulary size for the decoder (French)
max_encoder_seq_length = 20 # Max length of input sequences
max_decoder_seq_length = 20 # Max length of output sequences

# Encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]  # Keep encoder states

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states) # Use encoder states as initial decoder states
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Example dummy data (replace with your actual data)
import numpy as np
encoder_input_data = np.random.randint(0, num_encoder_tokens, size=(1000, max_encoder_seq_length))
decoder_input_data = np.random.randint(0, num_decoder_tokens, size=(1000, max_decoder_seq_length))
decoder_target_data = np.zeros((1000, max_decoder_seq_length, num_decoder_tokens))

# One-hot encode the decoder target data
for i in range(1000):
    for t in range(max_decoder_seq_length):
        decoder_target_data[i, t, decoder_input_data[i, t]] = 1.0  # next word, one-hot encoded

# Train the model
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=64,
          epochs=10,  # Adjust as needed
          validation_split=0.2)


# Inference Setup (separate models for encoder and decoder)

encoder_model = keras.Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_embedding, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)

# Usage example (inference - generating translations from new English sentences)
# (Needs proper vocabularies and tokenization; this is just conceptual)

# encoder_input_sentence = "This is a test" # Example English sentence

# def translate(input_sentence): # pseudo code
#    states_value = encoder_model.predict(tokenize(input_sentence))
#    target_seq = np.zeros((1, 1)) # Initialize with start of sequence token
#    stop_condition = False
#    decoded_sentence = ''
#    while not stop_condition:
#        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
#        sampled_token_index = np.argmax(output_tokens[-1, :])
#        decoded_word = reverse_lookup[sampled_token_index] # map back to the french word
#        decoded_sentence += decoded_word

#        if (decoded_word == '\n' or len(decoded_sentence) > max_decoder_seq_length):
#            stop_condition = True

#        target_seq = np.zeros((1, 1))
#        target_seq[0, 0] = sampled_token_index
#        states_value = [h, c]
#    return decoded_sentence
```

**Important Notes:**

*   This is a *very* basic example for demonstration.  It lacks crucial components for real-world translation, such as an attention mechanism.
*   You'll need to create proper tokenizers and vocabularies to map words to integers and vice versa.
*   Pre-trained word embeddings (e.g., Word2Vec, GloVe, or fastText) can significantly improve performance.
*   The inference (translation) part of the code is commented out because it requires the tokenizer and reverse lookup dictionaries which are not part of the core encoder-decoder structure, but are required for the practical usage of the model.

## 4) Follow-up question

How does the Attention Mechanism enhance the Encoder-Decoder architecture, and what problem does it solve?