<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TF-IDF (Term Frequency-Inverse Document Frequency)</title>
  <script>
    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
  </script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }

    .page-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 24px;
      padding-bottom: 16px;
      border-bottom: 1px solid #e5e7eb;
    }
    .back-link {
      font-weight: 600;
      text-decoration: none;
    }
    .back-link:hover {
      text-decoration: underline;
    }

    .theme-toggle {
      background: none;
      border: none;
      cursor: pointer;
      font-size: 1.5rem;
      padding: 8px;
      border-radius: 50%;
      transition: background 0.2s;
    }
    .theme-toggle:hover {
      background: rgba(0,0,0,0.05);
    }
    .dark .theme-toggle:hover {
      background: rgba(255,255,255,0.1);
    }

    .dark body {
      background: #0f172a;
      color: #e2e8f0;
    }
    .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
    .dark a { color: #60a5fa; }
    .dark .page-header { border-bottom-color: #334155; }
    .dark code { background-color: #1e293b; color: #e2e8f0; }
    .dark pre {
      border: 1px solid #334155;
    }
    .dark blockquote {
      border-left-color: #334155;
      color: #94a3b8;
    }
    .dark th, .dark td { border-color: #334155; }
    .dark th { background: #1e293b; }
    .dark hr { border-top-color: #334155; }
  </style>
</head>
<body>

  <div class="page-header">
    <a href="../../../../../hubs/nlp-index.html" class="back-link">‚Üê Back to Natural Language Processing</a>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle Dark Mode">üåô</button>
  </div>

<h1 id="topic-tf-idf-term-frequency-inverse-document-frequency">Topic: TF-IDF (Term Frequency-Inverse Document Frequency)</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It's a widely used technique in information retrieval and text mining to determine the significance of words within a document compared to the entire corpus.  It measures the relevance of a word to a document relative to a corpus.</p>
<p>TF-IDF is calculated by multiplying two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).</p>
<ul>
<li>
<p><strong>Term Frequency (TF):</strong>  Measures how frequently a term appears in a document. A common approach is to simply count the number of times a term appears in a document, divided by the total number of words in that document.  Other variations exist, such as logarithmic scaling to reduce the impact of very frequent words.</p>
<ul>
<li>TF(t,d) = (Number of times term <em>t</em> appears in document <em>d</em>) / (Total number of words in document <em>d</em>)</li>
</ul>
</li>
<li>
<p><strong>Inverse Document Frequency (IDF):</strong> Measures how important a term is across the entire corpus. It aims to reduce the weight of common words (e.g., "the", "a", "is") that appear frequently in almost all documents, while increasing the weight of rare words that are more specific to particular documents. It's typically calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term.</p>
<ul>
<li>IDF(t, D) = log(Total number of documents in corpus <em>D</em> / Number of documents containing term <em>t</em>)</li>
</ul>
</li>
</ul>
<p>The TF-IDF score is then calculated as:</p>
<ul>
<li>TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)</li>
</ul>
<p><strong>How can we use it?</strong></p>
<p>We use TF-IDF to:</p>
<ul>
<li><strong>Rank search results:</strong> Documents with higher TF-IDF scores for the query terms are considered more relevant.</li>
<li><strong>Feature extraction for text classification:</strong>  TF-IDF scores can be used as features for machine learning models to classify documents.  Higher scores represent more relevant words for the class.</li>
<li><strong>Information retrieval:</strong>  Identify the most important words in a document for indexing and retrieval purposes.</li>
<li><strong>Keyword extraction:</strong>  Identify important keywords within a document.</li>
<li><strong>Text summarization:</strong>  Identify sentences containing terms with high TF-IDF scores to form a summary.</li>
<li><strong>Document similarity:</strong> TF-IDF vectors can be used to compare the similarity between documents. Documents with similar TF-IDF vectors are considered more similar.</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Imagine you have a collection of articles about different sports: Soccer, Basketball, and Tennis.</p>
<ul>
<li>
<p><strong>Scenario:</strong> You want to build a search engine for these articles. When a user searches for "scoring strategies", you want to return the articles that are most relevant to that query.</p>
</li>
<li>
<p><strong>How TF-IDF helps:</strong> TF-IDF can be used to weigh the importance of the terms "scoring" and "strategies" in each article relative to the entire collection.  An article about basketball that heavily uses the term "scoring" and "strategies" (relative to other articles) will have a higher TF-IDF score for those terms.  Conversely, articles that mention those terms only briefly might be less relevant.</p>
</li>
<li>
<p><strong>Expected outcome:</strong> The search engine would return articles about Basketball (and perhaps Soccer) that discuss "scoring strategies" prominently, while articles about Tennis, or Basketball articles that don't focus on strategies, would be ranked lower.</p>
</li>
</ul>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<pre class="codehilite"><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

# Sample documents
documents = [
    &quot;This is the first document.&quot;,
    &quot;This document is the second document.&quot;,
    &quot;And this is the third one.&quot;,
    &quot;Is this the first document?&quot;,
    &quot;This is a document about scoring in basketball.  Scoring is very important.&quot;,
    &quot;Tennis is a sport that requires precise strokes and strategy.&quot;,
    &quot;Soccer is a sport that requires scoring goals using different strategies.&quot;
]

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the documents
tfidf_matrix = vectorizer.fit_transform(documents)

# Get the feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Convert the TF-IDF matrix to a dense array (for easier viewing)
dense = tfidf_matrix.todense()
denselist = dense.tolist()

# Print the TF-IDF scores for each document and each term
import pandas as pd
df = pd.DataFrame(denselist, columns=feature_names)
print(df)

# Example: Get the TF-IDF score for the term &quot;scoring&quot; in the 5th document
scoring_index = feature_names.tolist().index(&quot;scoring&quot;)
print(f&quot;\nTF-IDF score for 'scoring' in document 5: {denselist[4][scoring_index]}&quot;)

# To search for documents most relevant to the query &quot;scoring strategies&quot;:
query = &quot;scoring strategies&quot;
query_vector = vectorizer.transform([query])
#We can then calculate cosine similarity between query_vector and each row of tfidf_matrix
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_vector, tfidf_matrix)
print(&quot;\nCosine similarities between query and documents:&quot;)
print(similarities)
</code></pre>

<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>TF-IDF is a relatively simple and computationally efficient method. What are some of its limitations, and what are some alternative, more advanced techniques that address those limitations?</p>
  <script>
    const btn = document.getElementById('theme-toggle');
    const html = document.documentElement;
    
    function updateIcon() {
      btn.textContent = html.classList.contains('dark') ? '‚òÄÔ∏è' : 'üåô';
    }
    
    // Set initial icon
    updateIcon();

    btn.addEventListener('click', () => {
      if (html.classList.contains('dark')) {
        html.classList.remove('dark');
        localStorage.theme = 'light';
      } else {
        html.classList.add('dark');
        localStorage.theme = 'dark';
      }
      updateIcon();
    });
  </script>
</body>
</html>
