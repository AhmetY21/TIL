---
title: "N-Grams and Language Models"
date: "2026-02-14"
week: 7
lesson: 1
slug: "n-grams-and-language-models"
---

# Topic: N-Grams and Language Models

## 1) Formal definition (what is it, and how can we use it?)

**N-grams:** An n-gram is a contiguous sequence of *n* items from a given sample of text or speech. The "items" can be characters, syllables, words, or any definable unit. So, a 1-gram (or unigram) is a single word, a 2-gram (or bigram) is a sequence of two words, a 3-gram (or trigram) is a sequence of three words, and so on.

**Language Models:** A language model (LM) is a probability distribution over sequences of words.  In simpler terms, it tries to assign a probability to a sentence, indicating how likely that sentence is to occur in the language it was trained on. LMs use n-grams to approximate this probability. The probability of a sentence *S* = *w1, w2, ..., wn* can be estimated using the chain rule of probability:

P(S) = P(w1, w2, ..., wn) = P(w1) * P(w2|w1) * P(w3|w1, w2) * ... * P(wn|w1, w2, ..., wn-1)

This conditional probability becomes computationally expensive and difficult to estimate directly, especially for long-range dependencies.  N-gram models simplify this using the *Markov assumption*, which states that the probability of a word depends only on the preceding *n-1* words:

P(wi | w1, w2, ..., wi-1) ≈ P(wi | wi-n+1, wi-n+2, ..., wi-1)

Therefore, an n-gram language model estimates the probability of a word given the preceding n-1 words. Common n-gram models include:

*   **Unigram Model (n=1):** P(wi | w1, w2, ..., wi-1) ≈ P(wi)
*   **Bigram Model (n=2):** P(wi | w1, w2, ..., wi-1) ≈ P(wi | wi-1)
*   **Trigram Model (n=3):** P(wi | w1, w2, ..., wi-1) ≈ P(wi | wi-2, wi-1)

**How we can use it:**

N-grams and language models are used for:

*   **Text Generation:** Generating plausible text sequences.
*   **Speech Recognition:** Evaluating the likelihood of word sequences.
*   **Machine Translation:**  Scoring candidate translations.
*   **Spell Checking:**  Suggesting corrections for misspelled words.
*   **Text Summarization:** Identifying important phrases.
*   **Autocompletion/Predictive Text:** Predicting the next word a user is likely to type.
*   **Sentiment Analysis:** Using n-grams as features to classify sentiment.

## 2) Application scenario

Consider a scenario where you are building an autocompletion feature for a search engine. When a user types "I want to...", you want to suggest possible completions based on the most likely words to follow that phrase. You can train a bigram language model on a large corpus of text (e.g., web pages, books, articles).

The bigram model would estimate probabilities like P("pizza" | "to"), P("eat" | "to"), P("go" | "to"), etc., based on the frequencies observed in the training data.  The system would then suggest the words with the highest probabilities, effectively predicting what the user is most likely to type next.  For example, if P("pizza" | "to") and P("go" | "to") are much higher than other words, the autocompletion might suggest "pizza" or "go".

## 3) Python method (if possible)

```python
from nltk.util import ngrams
from nltk import word_tokenize
from collections import Counter

def create_ngram_model(text, n):
  """
  Creates an n-gram model from the given text.

  Args:
    text: The input text as a string.
    n: The order of the n-gram (e.g., 2 for bigrams, 3 for trigrams).

  Returns:
    A dictionary where keys are n-grams (tuples of words) and values are their counts.
  """
  tokens = word_tokenize(text.lower()) # Tokenize and lowercase the text
  ngrams_list = list(ngrams(tokens, n)) # Generate n-grams
  ngram_counts = Counter(ngrams_list)  # Count the occurrences of each n-gram
  return ngram_counts

def calculate_ngram_probability(ngram, ngram_counts, lower_order_counts, vocabulary_size):
  """Calculates the probability of an n-gram using add-k smoothing.
  Assumes ngram_counts and lower_order_counts are generated by create_ngram_model.
  """
  k = 0.01 # Add-k smoothing parameter
  count_ngram = ngram_counts.get(ngram, 0)
  if len(ngram) > 1: # For n > 1, use conditional probability
      lower_order_ngram = ngram[:-1] #Get the n-1 gram prefix
      count_lower_order = lower_order_counts.get(lower_order_ngram,0)
  else: #For unigrams, we're looking at the relative frequency.
      count_lower_order = len(word_tokenize(example_text.lower()))

  if count_lower_order == 0:
      return k / (vocabulary_size * k) #Handles cases where the lower order ngram has a zero count

  probability = (count_ngram + k) / (count_lower_order + k * vocabulary_size)
  return probability

# Example Usage
example_text = "I want to eat pizza. I want to go home. Let's eat pizza."
bigram_counts = create_ngram_model(example_text, 2)
unigram_counts = create_ngram_model(example_text, 1)
vocabulary = set(word_tokenize(example_text.lower()))
vocabulary_size = len(vocabulary)


probability_pizza = calculate_ngram_probability(('pizza',), unigram_counts, {}, vocabulary_size) #Calculate P(pizza)
probability_eat_pizza = calculate_ngram_probability(('eat','pizza'), bigram_counts, unigram_counts, vocabulary_size) #Calculate P(pizza | eat)

print(f"Bigram Counts: {bigram_counts}")
print(f"Unigram Counts: {unigram_counts}")
print(f"P(pizza): {probability_pizza}")
print(f"P(pizza | eat): {probability_eat_pizza}")

```

This code uses the `nltk` library for tokenization and n-gram generation and the `collections.Counter` class for counting n-gram occurrences. The `create_ngram_model` function takes text and the n-gram order as input and returns a dictionary containing the counts of each n-gram. The `calculate_ngram_probability` calculates the probability of a n-gram. Add-k smoothing prevents zero probabilities when the n-gram hasn't been seen in the training data. It assumes the counts for ngrams and n-1 grams (needed for conditional probability) are calculated beforehand and also the vocabulary size.

## 4) Follow-up question

How can we handle unseen n-grams (n-grams not present in the training data) more effectively than with simple add-k smoothing, and what are some common techniques used for this purpose (e.g., Good-Turing smoothing, Kneser-Ney smoothing)? How do these techniques improve the performance of language models?