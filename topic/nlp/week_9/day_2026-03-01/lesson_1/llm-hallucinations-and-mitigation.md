---
title: "LLM Hallucinations and Mitigation"
date: "2026-03-01"
week: 9
lesson: 1
slug: "llm-hallucinations-and-mitigation"
---

# Topic: LLM Hallucinations and Mitigation

## 1) Formal definition (what is it, and how can we use it?)

LLM hallucinations, in the context of Natural Language Processing (NLP), refer to instances where a Large Language Model (LLM) generates content that is factually incorrect, nonsensical, or irrelevant to the given prompt, despite appearing coherent and confident. Essentially, it's when the LLM "makes things up." These fabrications can manifest as inventing facts, attributing false information to sources, creating non-existent concepts, or providing answers that are internally inconsistent or contradict widely accepted knowledge.

More formally, hallucinations can be categorized into two main types:

*   **Factuality Hallucinations:**  The LLM generates statements that directly contradict verifiable facts in the real world.  For example, "The capital of France is Berlin."

*   **Faithfulness Hallucinations:** The LLM generates content that contradicts the source document it is supposed to be summarizing or answering questions about. This is particularly relevant in retrieval-augmented generation (RAG) setups. For example, if a document states "Company X increased revenue by 10%", the LLM hallucinates by stating "Company X decreased revenue by 5%."

How can we use the *concept* of hallucinations?

*   **Evaluating LLM Reliability:** Identifying and quantifying the presence of hallucinations is crucial for assessing the trustworthiness and reliability of an LLM.
*   **Developing Mitigation Strategies:**  Understanding the causes of hallucinations is the first step in developing methods to reduce their occurrence.
*   **Building Trustworthy AI Systems:** By minimizing hallucinations, we can increase user confidence in LLM-powered applications across various domains.
*   **Identifying Model Limitations:** Hallucinations highlight the inherent limitations of LLMs, particularly their reliance on statistical patterns rather than genuine understanding.
*   **Improving Prompt Engineering:** Knowing that LLMs can hallucinate encourages more careful and precise prompt engineering to guide the model towards factual and relevant responses.

## 2) Application scenario

Consider a customer service chatbot powered by an LLM. A user asks: "What is the return policy for the Acme Widget 3000?"

Without sufficient grounding in the correct documentation, the LLM might hallucinate the following response:

"For the Acme Widget 3000, you can return it within 60 days for a full refund, no questions asked. You'll also receive a free Acme Widget polishing cloth upon returning the product."

However, the *actual* return policy might be: 30 days for a refund, with a 15% restocking fee, and no mention of a polishing cloth.

In this scenario, the LLM is hallucinating a more lenient and attractive return policy, which could lead to:

*   **Customer dissatisfaction:**  The customer relies on the incorrect information and experiences frustration when the actual policy is different.
*   **Loss of trust:** The customer loses trust in the chatbot and the company providing it.
*   **Operational issues:**  The company's customer service team has to deal with complaints stemming from the inaccurate information provided by the chatbot.
*   **Legal liabilities:** In certain regulated industries, providing incorrect information could lead to legal issues.

This scenario highlights the importance of mitigating hallucinations, especially in applications that require accurate and reliable information. Retrieval-augmented generation (RAG) becomes crucial here, where the LLM is given access to the actual return policy document to inform its answer.

## 3) Python method (if possible)

While there isn't a single "hallucination detector" function that works perfectly, we can use Python libraries and techniques to assess the *likelihood* of hallucinations, particularly in the context of RAG or summarization.  Here's an example using the `transformers` library to measure the consistency between a generated summary and the source document. This doesn't directly *detect* hallucinations but gives an indication of potential inaccuracies.

```python
from transformers import pipeline
from rouge import Rouge

def evaluate_summary_faithfulness(source_text, generated_summary):
    """
    Evaluates the faithfulness of a generated summary to the source text using ROUGE scores.
    Higher ROUGE scores indicate better faithfulness.

    Args:
        source_text: The original text document.
        generated_summary: The summary generated by the LLM.

    Returns:
        A dictionary containing ROUGE scores.
    """

    rouge = Rouge()
    try:
      scores = rouge.get_scores(generated_summary, source_text)[0] #rouge expects lists
      return scores
    except ValueError as e:
      print(f"Error calculating ROUGE scores: {e}")
      return {"rouge-1": {"f": 0.0, "p": 0.0, "r": 0.0},
              "rouge-2": {"f": 0.0, "p": 0.0, "r": 0.0},
              "rouge-l": {"f": 0.0, "p": 0.0, "r": 0.0}}
    # Example Usage:
    source_text = "The quick brown fox jumps over the lazy dog. The dog barks loudly."
    generated_summary = "A fox jumps over a dog."
    rouge_scores = evaluate_summary_faithfulness(source_text, generated_summary)
    print(f"ROUGE Scores: {rouge_scores}")


    source_text = "Company X increased revenue by 10% in Q3. Profits also rose by 5%."
    hallucinated_summary = "Company X decreased revenue by 5% in Q3 and increased profits by 20%"
    rouge_scores_hallucinated = evaluate_summary_faithfulness(source_text, hallucinated_summary)
    print(f"ROUGE Scores for Hallucinated Summary: {rouge_scores_hallucinated}")
```

**Explanation:**

1.  **`evaluate_summary_faithfulness(source_text, generated_summary)`:**
    *   Takes the source text and the generated summary as input.
    *   Uses the `Rouge` class from the `rouge` library to calculate ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores. ROUGE is a set of metrics commonly used to evaluate the quality of summaries. Higher ROUGE scores generally indicate better similarity between the summary and the source text, suggesting higher faithfulness and lower likelihood of hallucinations.
    *   Calculates ROUGE-1, ROUGE-2, and ROUGE-L scores. These metrics measure the overlap of unigrams (ROUGE-1), bigrams (ROUGE-2), and the longest common subsequence (ROUGE-L) between the summary and the source text.
    *   Returns a dictionary containing the ROUGE scores.
    *   Includes error handling for potential issues during ROUGE score calculation.

**Limitations:**

*   ROUGE scores don't directly detect hallucinations, but rather measure similarity. A hallucinated summary might still have high ROUGE scores if it uses similar wording to the source text while being factually incorrect.
*   This method is more applicable to summarization tasks or RAG-based question answering where you have a source document to compare against.
*   More sophisticated methods involve using dedicated hallucination detection models (which are still an active area of research).  These often involve knowledge graphs and fact verification techniques.

## 4) Follow-up question

Given that ROUGE scores, as demonstrated above, provide an imperfect measure of detecting hallucinations, what other metrics or techniques are being developed to more accurately identify and quantify LLM hallucinations, particularly in open-ended text generation scenarios where no direct source document is available?  Consider approaches that incorporate external knowledge sources or internal model consistency checks.